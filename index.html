<html>
<head>
<style>

.hide { display: none; }

</style>
</head>
<body>

<h1>multisync</h1>
<h1 id="status"></h1>
<div id="loading">connecting to server...</div>
<div id="loaded">
	<div id="req_access"><strong>press this button --></strong> <button onclick="requestAudioAccess()">request audio access</button></div>
	<span id="online">loading online users...</span>
	<input id="username" type="text" placeholder="username" />
	<button onclick="changeUsername()">change username</button>
	<hr>

	<span id="bpm">loading bpm...</span>
	<input id="bpm_in" type="text" placeholder="bpm" />
	<button onclick="changeBPM()">change bpm for countdown</button>
	<br><br>

	<span id="cnt">loading counts...</span>
	<input id="cnt_in" type="text" placeholder="counts" />
	<button onclick="changeCounts()">change counts for countdown</button>
	<br><br>
	
	<p id="backing">loading backing track...</p>
	<audio controls id="backing_au"></audio><br>
	<input id="backing_in" type="file" />
	<button onclick="changeBacking()">change backing track</button>
	<br><br>
	<input id="metro_in" type="text" placeholder="bpm" />
	<input id="metro_sig_in" type="text" placeholder="time signature" />/4
	<button onclick="changeMetro()">use metronome backing track</button>
	<br><br><button onclick="clearBacking()">clear backing track (not recommended if multiple people are playing)</button>
	<br><br><span id="audience">loading backing track distribution...</span>
	<button onclick="changeAudience(true)">audience and performers hear backing track</button>
	<button onclick="changeAudience(false)">only performers hear backing track</button>
	<!-- <hr>
	<h2>warning: compression is broken right now</h2>
	<span id="compression">not using compression</span>
	<button onclick="changeCompression(true)">enable compression</button>
	<button onclick="changeCompression(false)">disable compression</button>
	<hr>
	
	<span id="input_off">input offset: 0ms</span>
	<input id="off_in" type="text" placeholder="offset" />
	<button onclick="changeOffset()">change input offset (ms, more = earlier)</button>
	
	<br><br><button onclick="beginSync()">begin sync</button>
	<button onclick="endSync()">end sync</button>
	<input id="sync_earlier" type="text" placeholder="ms earlier">
	<button onclick="playSync()">play sync</button>
	<button onclick="endPlaySync()">stop playing sync</button>
	<hr>-->
	<h2>sync</h2>
	<p>try to line up the waveform to the <strong>beginning</strong> and <strong>middle</strong> of this preview</p>
	<p id="sync_user_header"></p>
	<ol id="sync_user_list"></ol>
	<canvas id="sync_vis"></canvas><br>
	<input id="target_in" type="text" placeholder="username" />
	<input id="sync_in" type="text" placeholder="sync">
	<button onclick="syncUser()">make user earlier (ms) (leave username blank for you)</button>
	<br>sync volume: <input id="sync_vol" type="range" min="0" max="1" step="any" value="0.8" onchange="setSyncVolume()" />
	<br>
	<h2>advanced stuff</h2>
	<button onclick="soloUser()">stop multisync; just play the audio for the user typed above (leave username blank for you)</button>
	<br><br><span id="sbuf">loading minimum sample buffer size...</span>
	<input id="sbuf_in" type="text" placeholder="minimum sample buffer size">
	<button onclick="changeMinBuf()">set minimum sample buffer size (ms) (more = playback starts later for audience, but more lag tolerance)</button>
	<br><button onclick="stop(true)">stop for everyone, don't wait for buffer (will cut off a bit of the audio)</button>
	<hr>
	<h1 id="rec_status">status: currently not live</h1>
	<button onclick="start()">start for everyone</button>
	<button onclick="stop(false)">stop for everyone</button>
	<button onclick="stopSync()">stop syncing and begin live performance! (only do when synced)</button>
</div>

<script>
	window.bufferSize = 8192;
	window.audioContext = new AudioContext({
		sampleRate: 48000
	});
	if(window.audioContext.sampleRate !== 48000) alert("Your browser does not support the required sample rate, instead " + window.audioContext.sampleRate);
	audioContext.audioWorklet.addModule("./server_worklet.js");
</script>
<script src="./ws-audio-api.min.js"></script>
<script>

let status = document.getElementById("status");
if(!navigator.getUserMedia) {
	status.innerHTML = "error: can't get microphone input. check your settings (chrome://flags/#unsafely-treat-insecure-origin-as-secure, edge://flags/#unsafely-treat-insecure-origin-as-secure)";
}

let loading = document.getElementById("loading");
let loaded = document.getElementById("loaded");

let onlineUsers = document.getElementById("online");
let usernameInput = document.getElementById("username");

let bpm = document.getElementById("bpm");
let bpmInput = document.getElementById("bpm_in");
let counts = document.getElementById("cnt");
let countsInput = document.getElementById("cnt_in");
let compression = document.getElementById("compression");
let backing = document.getElementById("backing");
let backingAudio = document.getElementById("backing_au");
let backingInput = document.getElementById("backing_in");
let offsetInput = document.getElementById("off_in");
let soloInput = document.getElementById("target_in");
let syncInput = document.getElementById("sync_in");
let syncEarlier = document.getElementById("sync_earlier");
let canvas = document.getElementById("sync_vis");
let ctx = canvas.getContext("2d");
let minBuf = document.getElementById("sbuf");
let minBufInput = document.getElementById("sbuf_in");
let metroInput = document.getElementById("metro_in");
let metroSigInput = document.getElementById("metro_sig_in");
let audience = document.getElementById("audience");
let recStatus = document.getElementById("rec_status");
let syncVol = document.getElementById("sync_vol");
let syncUserHeader = document.getElementById("sync_user_header");
let syncUserList = document.getElementById("sync_user_list");
let backingData = null;
let backingNode = null;
let metronomeBPM = 0;
let metronomeSig = 0;
let sampleWait = 0;
let syncs = [];
let syncing = false;
let syncIndex = 0;
loaded.className = "hide";

usernameInput.value = "anon" + (Math.random() * 100000 >>> 0);

let ws = new WebSocket("ws://" + location.hostname + ":3000");
ws.binaryType = "blob";

ws.addEventListener("open", e => {
	loading.className = "hide";
	loaded.className = "";
	changeUsername();
});

ws.addEventListener("error", e => {
	loading.className = "";
	loaded.className = "hide";
	loading.innerText = "failed to connect to server";
});

ws.addEventListener("close", e => {
	loading.className = "";
	loaded.className = "hide";
	loading.innerText = "connection to server was closed";
});

function handleSyncMessage(msg) {
	if(msg === "newf") {
		syncIndex = 0;
	}
	if(msg.startsWith("stop")) {
		streamer.recorder.port.postMessage({
			syncBeat: true,
			val: parseInt(msg.slice(4)) * 2
		});
	}
}

function handleSyncBin(x) {
	let nums = new Int8Array(x);
	syncs[syncIndex] = { max: nums.subarray(0, nums.length >> 1), min: nums.subarray(nums.length >> 1) };
	syncIndex++;
}

ws.addEventListener("message", e => {
	console.log(e.data);
	if(e.data instanceof ArrayBuffer) {
		if(syncing) {
			handleSyncBin(e.data);
			return;
		}
		backingAudio.src = URL.createObjectURL(new Blob([e.data]));
		backingAudio.load();
		backing.innerText = "decoding backing track";
		audioContext.decodeAudioData(e.data, d => {
			backing.innerText = "decoded backing track";
			setTimeout(() => {
				backing.innerText = "";
			}, 1000);
			backingData = d;
		}, () => {
			if(e.data.byteLength === 0) {
				backing.innerText = "backing track is empty";
			} else {
				backing.innerText = "failed to decode backing track";
			}
			backingData = null;
		});
	} else if(e.data instanceof Blob) {
		if(syncing) {
			e.data.arrayBuffer().then(x => {
				handleSyncBin(x);
			});
			return;
		}
		backing.innerText = "loading backing track";
		backingAudio.src = URL.createObjectURL(e.data);
		backingAudio.load();
		e.data.arrayBuffer().then(x => {
			backing.innerText = "decoding backing track";
			audioContext.decodeAudioData(x, d => {
				backing.innerText = "decoded backing track";
				setTimeout(() => {
					backing.innerText = "";
				}, 1000);
				backingData = d;
			}, () => {
				if(x.byteLength === 0) {
					backing.innerText = "backing track is empty";
				} else {
					backing.innerText = "failed to decode backing track";
				}
				backingData = null;
			});
		});
	} else if(e.data.startsWith("sync")) {
		handleSyncMessage(e.data.slice(4));
	} else if(e.data.startsWith("onl:")) {
		onlineUsers.innerText = "online users: " + e.data.slice(4);
	} else if(e.data.startsWith("bpm:")) {
		bpm.innerText = "bpm: " + e.data.slice(4);
	} else if(e.data.startsWith("cnt:")) {
		counts.innerText = "counts: " + e.data.slice(4);
	} else if(e.data.startsWith("buf:")) {
		minBuf.innerText = "minimum sample buffer size: " + parseInt(e.data.slice(4)) / 48 + "ms";
	} else if(e.data.startsWith("bck:")) {
		backing.innerText = "backing track is changing, please wait...";
	} else if(e.data.startsWith("idx:")) {
		let userIdx = e.data.slice(4).split(";");
		syncUserList.innerHTML = "";
		for(let i = 0; i < userIdx.length; i++) {
			let el = document.createElement("li");
			el.innerText = userIdx[i];
			syncUserList.appendChild(el);
		}
		syncUserHeader.innerText = "this is the order of the waveforms below.";
	} else if(e.data.startsWith("abk:")) {
		if(e.data[4] === "t") {
			audience.innerText = "audience and performers hear backing track";
		} else {
			audience.innerText = "only performers hear backing track";
		}
	} else if(e.data.startsWith("met:")) {
		backing.innerText = "backing track is being generated, plese wait...";
		let parts = e.data.split(":");
		metronomeBPM = Number(parts[1]);
		metronomeSig = parseInt(parts[2]);
		let pcm = generateMetronome(metronomeBPM, metronomeSig);
		let wav = generateWav(pcm);
		backingAudio.src = URL.createObjectURL(wav);
		backingAudio.load();
		backing.innerText = "generated backing track";
		setTimeout(() => {
			backing.innerText = "";
		}, 1000);
		backingData = pcm;
	} else if(e.data.startsWith("srt:")) {
		sampleWait = parseInt(e.data.slice(4));
		syncing = true;
		streamer.samplesWritten = 0;
		let backingWaitCount = parseInt(counts.innerText.slice(8));
		let w = sampleWait / backingWaitCount;
		streamer.backing = {
			backingData: backingData && backingData.getChannelData(0),
			backingWait: w,
			backingWaitCount,
			vol: syncVol.value,
		};
		streamer.recStatus = recStatus;
		streamer.start();
		recStatus.innerText = "status: syncing";
	} else if(e.data.startsWith("stp:")) {
		streamer.stop();
		ws.send("end:");
	}
});

var streamer = new WSAudioAPI.Streamer({
	//    App: 2048=voip, 2049=audio, 2051=low-delay
	//    Sample Rate: 8000, 12000, 16000, 24000, or 48000
	//    Frame Duration: 2.5, 5, 10, 20, 40, 60
	//    Buffer Size = sample rate/6000 * 1024
	codec: {
		sampleRate: 48000,
		channels: 1,
		app: 2049,
		frameDuration: 20,
		bufferSize: window.bufferSize,
	},
}, ws);

function generateMetronome(bpm, sig) {
	let amt = 2000;
	let out = new AudioBuffer({
		length: 48000 * 60 * amt / bpm,
		sampleRate: 48000,
		numberOfChannels: 1
	});
	let buf = out.getChannelData(0);
	let tick = j => {
		for(let i = 0; i < 2400; i++) {
			buf[j + i] = i % 48 >= 24 ? 0.8 : -0.8;
		}
	};
	let uptick = j => {
		for(let i = 0; i < 2400; i++) {
			buf[j + i] = i % 24 >= 12 ? 0.8 : -0.8;
		}
	};
	for(let i = 0; i < amt; i++) {
		let sample = ((48000 * 60 * i) / bpm) >>> 0;
		let downbeat = i % sig === 0;
		if(downbeat) uptick(sample); else tick(sample);
	}
	return out;
}

function generateWav(pcm) {
	let int = n => new Uint32Array([n]);
	let short = n => new Uint16Array([n]);
	// http://soundfile.sapp.org/doc/WaveFormat/
	return new Blob([
		// ChunkID, ChunkSize, Format
		"RIFF", int(50 + pcm.length), "WAVE",
		
		// Subchunk1ID, Subchunk1Size, AudioFormat
		"fmt ", int(18), short(3),
		// NumChannels, SampleRate, ByteRate
		short(1), int(48000), int(48000 * 4),
		// BlockAlign, BitsPerSample, ExtensionSize
		short(4), short(32), short(0),
		
		// Subchunk2ID, Subchunk2Size, SamplesPerChannel
		"fact", int(4), int(pcm.length),

		// Subchunk3ID, Subchunk3Size
		"data", int(pcm.length * 4),
		// Data
		pcm.getChannelData(0)
	], { type: "audio/wav" });
}

function start() {
	if(backingData == null && !confirm("no backing track is selected, are you sure?")) return;
	ws.send("srt:");
}

function stop(a) {
	ws.send("stp:" + a);
}

function changeUsername() {
	ws.send("usr:" + usernameInput.value);
}

function changeBPM() {
	ws.send("bpm:" + bpmInput.value);
}

function changeCounts() {
	ws.send("cnt:" + countsInput.value);
}

function changeCompression(v) {
	if(v) {
		ws.send("ops:");
		compression.innerText = "using compression";
	} else {
		ws.send("nps:");
		compression.innerText = "not using compression";
	}
	streamer.useCompression = v;
}

function changeBacking() {
	ws.send("bck:");
	backing.innerText = "sending backing track, please wait...";
	ws.send(backingInput.files[0]);
}

function changeMetro() {
	ws.send("met:" + metroInput.value + ":" + metroSigInput.value);
}

function changeOffset() {
	ws.send("off:" + (parseInt(offsetInput.value) * 48));
}

function soloUser() {
	ws.send("emg:" + soloInput.value);
}

function syncUser() {
	let samples = syncInput.value * 48;
	if(samples < 0) { samples += 0x10000; }
	let v = 0x1000000 + (samples << 8);
	ws.send("emg:" + soloInput.value + ":" + v);
}

function changeMinBuf() {
	ws.send("buf:" + (parseInt(minBufInput.value) * 48));
}

function changeAudience(hears) {
	ws.send(hears ? "abk:" : "nbk:");
}

function clearBacking() {
	ws.send(new ArrayBuffer(0));
}

function stopSync() {
	ws.send("sts:");
}

function setSyncVolume() {
	if(streamer.recorder != null) streamer.recorder.port.postMessage({ syncBeat: false, val: syncVol.value });
}

function requestAudioAccess() {
	audioContext.resume();
	document.getElementById("req_access").className = "hide";
}

/*let syncAudioInput = null;
let syncProcessor = null;
let syncRecorded = null;
let syncRecordIndex = 0;
let didntShowSyncBufferAlert = true;

function beginSync() {
	audioContext.resume();
	if(backingData == null) {
		alert("backing track is required for sync testing");
		return;
	}
	navigator.getUserMedia(
		{
			audio: {
				latency: 0,
				echoCancellation: false,
				mozNoiseSuppression: false,
				mozAutoGainControl: false
			}
		},
		function (stream) {
			syncRecorded = new Float32Array(backingData.length); // only 1 channel
			syncRecordIndex = 0;
			didntShowSyncBufferAlert = true;
			syncAudioInput = audioContext.createMediaStreamSource(stream);
			syncProcessor = new AudioWorkletNode(audioContext, "sync-streamer", {
				processorOptions: backingData.getChannelData(0),
				channelCount: 1,
				channelCountMode: "explicit"
			});
			syncProcessor.port.onmessage = e => {
				syncRecorded.set(e.data, syncRecordIndex);
				syncRecordIndex += e.data.length;
			};
			syncAudioInput.connect(syncProcessor);
			syncProcessor.connect(audioContext.destination);
		},
		alert
	);

}

function endSync() {
	syncAudioInput.disconnect();
	syncAudioInput = null;
	syncProcessor.disconnect();
	syncProcessor = null;
}

let syncPlaybackNode = null;

function playSync() {
	let offset = parseInt(syncEarlier.value || 0) * 48;
	let playback = new AudioBuffer({
		length: backingData.length,
		sampleRate: backingData.sampleRate,
		numberOfChannels: backingData.numberOfChannels,
	});
	for(let c = 0; c < backingData.numberOfChannels; c++) {
		let data = playback.getChannelData(c);
		let base = backingData.getChannelData(c);
		for(let i = 0; i < backingData.length; i++) {
			data[i] = base[i];
			if(i < syncRecorded.length - offset) data[i] += syncRecorded[i + offset];
		}
	}
	if(syncPlaybackNode != null) syncPlaybackNode.stop();
	syncPlaybackNode = audioContext.createBufferSource();
	syncPlaybackNode.buffer = playback;
	syncPlaybackNode.connect(audioContext.destination);
	syncPlaybackNode.start();
}

function endPlaySync() {
	if(syncPlaybackNode == null) return;
	syncPlaybackNode.stop();
	syncPlaybackNode = null;
}*/

let previewSize = 1;
let previewOffset = 0;
function draw() {
	requestAnimationFrame(draw);
	ctx.fillStyle = "#eee";
	ctx.fillRect(0, 0, canvas.width, canvas.height);
	
	ctx.fillStyle = "#00ff00";
	ctx.globalAlpha = 0.5;
	ctx.fillRect(canvas.width / 2, 0, canvas.width, canvas.height);
	ctx.globalAlpha = 1;

	ctx.strokeStyle = "#000";
	ctx.lineWidth = 1;
	for(let i = 0; i < syncs.length; i++) {
		drawWaveform(syncs[i].max, syncs[i].min, syncs.length, i);
	}
}

requestAnimationFrame(draw);

function drawWaveform(maxs, mins, divs, divIdx) {
	let divSize = canvas.height / divs;
	let divCenter = divSize * divIdx + divSize / 2;
	let mult = maxs.length / canvas.width;
	for(let x = 0; x < canvas.width; x++) {
		let start = Math.round(x * mult * previewSize + previewOffset);
		let end = Math.round(start + mult * previewSize);

		let max = Math.max(...maxs.slice(start, end)) / 128;
		let min = Math.min(...mins.slice(start, end)) / 128;
		let upY = divCenter - max * divSize / 2;
		let downY = divCenter - min * divSize / 2;
		ctx.beginPath();
		ctx.moveTo(x, upY);
		ctx.lineTo(x, downY);
		ctx.stroke();
	}
}

/*function test() {
	audioContext.resume();
	if(backingData == null) {
		alert("backing track is required for sync testing");
		return;
	}
	navigator.getUserMedia(
		{
			audio: {
				latency: 0,
				echoCancellation: false,
				mozNoiseSuppression: false,
				mozAutoGainControl: false
			}
		},
		function (stream) {
			let audioInput = audioContext.createMediaStreamSource(stream);
			let processor = audioContext.createScriptProcessor(window.bu, 1, 1);
			let idx = 0;
			window.testBuffer = new Float32Array(480000);
			processor.onaudioprocess = function(e) {
				if(idx > window.testBuffer.length - e.inputBuffer.length) return;
				e.inputBuffer.copyFromChannel(window.testBuffer.subarray(idx, idx + e.inputBuffer.length), 0);
				idx += e.inputBuffer.length;
				
			}
			processor.connect(audioContext.destination);
			audioInput.connect(processor);
		},
		alert
	);
}*/

function download(buf) {
	let a = document.createElement("a");
	a.href = URL.createObjectURL(new Blob([buf], { type: "application/octet-stream" }));
	a.click();
	return a.href;
}
</script>
</body>
</html>
